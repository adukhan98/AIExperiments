{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adukhan98/AIExperiments/blob/main/VoiceChatbot_Using_Gradio%2C_Whisper%2C_OpenAi_and_eleven_Labs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akKo1idoWRpT"
      },
      "source": [
        "# Simple Voice Chatbot with OpenAI API, Gradio and Whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcpnBQx6WXlh"
      },
      "source": [
        "## ⚙️ Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0R3w04uNV__q"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q openai-whisper\n",
        "!pip install -q gradio\n",
        "!pip install -q openai==0.28\n",
        "!pip install -q elevenlabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxZmHCkaXQA0"
      },
      "source": [
        "## 🔌 Configure OpenAI and ElevenLabs Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRykoQ5HXU6l",
        "outputId": "27b86df1-b930-483f-faf8-22fc61db0aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass('Enter your OpenAI API Key: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buB1L5nv_Edq",
        "outputId": "94ff049a-9be9-4d47-81f3-945bbe8bbb0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your ElevenLabs API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "from elevenlabs import set_api_key\n",
        "from getpass import getpass\n",
        "\n",
        "set_api_key(getpass('Enter your ElevenLabs API Key: '))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjCxb4Bs07zp"
      },
      "source": [
        "## 🔊 Initiate Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NoONXCIA1Ec-"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YI1YGsRXvV_"
      },
      "source": [
        "## 🚀 Main Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "og1A3RKpWn8B"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "\n",
        "class Chat:\n",
        "\n",
        "    def __init__(self, system: Optional[str] = None):\n",
        "        self.system = system\n",
        "        self.messages = []\n",
        "\n",
        "        if system is not None:\n",
        "            self.messages.append({\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system\n",
        "            })\n",
        "\n",
        "    def prompt(self, content: str) -> str:\n",
        "          self.messages.append({\n",
        "              \"role\": \"user\",\n",
        "              \"content\": content\n",
        "          })\n",
        "          response = chat.completions.create(\n",
        "              model=\"gpt-3.5-turbo\",\n",
        "              messages=self.messages\n",
        "          )\n",
        "          response_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "          self.messages.append({\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": response_content\n",
        "          })\n",
        "          return response_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "l6REbRI-X2va",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04368e8f-3d64-4933-e622-47b80004c48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://0e5a26f564da7bc816.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0e5a26f564da7bc816.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 673, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-19-1125475febbc>\", line 10, in run_text_prompt\n",
            "    bot_message = chat.prompt(content=message)\n",
            "  File \"<ipython-input-18-d130ddf539be>\", line 21, in prompt\n",
            "    response = chat.completions.create(\n",
            "AttributeError: 'Chat' object has no attribute 'completions'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1522, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1144, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 673, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-19-1125475febbc>\", line 10, in run_text_prompt\n",
            "    bot_message = chat.prompt(content=message)\n",
            "  File \"<ipython-input-18-d130ddf539be>\", line 21, in prompt\n",
            "    response = chat.completions.create(\n",
            "AttributeError: 'Chat' object has no attribute 'completions'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
            "    response = await self.call_prediction(awake_events, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
            "    raise Exception(str(error) if show_error else None) from error\n",
            "Exception: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://0e5a26f564da7bc816.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from elevenlabs import generate, play\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "chat = Chat(system=\"You are a helpful assistant.\")\n",
        "\n",
        "\n",
        "def run_text_prompt(message, chat_history):\n",
        "    bot_message = chat.prompt(content=message)\n",
        "\n",
        "    audio = generate(\n",
        "        text=bot_message,\n",
        "        voice=\"Bella\"\n",
        "    )\n",
        "\n",
        "    play(audio, notebook=True)\n",
        "\n",
        "    chat_history.append((message, bot_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "\n",
        "def run_audio_prompt(audio, chat_history):\n",
        "    if audio is None:\n",
        "        return None, chat_history\n",
        "\n",
        "    message_transcription = model.transcribe(audio)[\"text\"]\n",
        "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
        "    return None, chat_history\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "\n",
        "    msg = gr.Textbox()\n",
        "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "    with gr.Row():\n",
        "        audio = gr.Audio( type=\"filepath\")\n",
        "\n",
        "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
        "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOvvLSlJrkQm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}